name: TMD Scraper

on:
  schedule:
    - cron: "0 13 * * *"   # 20:00 à¹„à¸—à¸¢ (13:00 UTC)
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    env:
      TZ: Asia/Bangkok
      CSV_OUT: tmd_7day_forecast_today.csv

    steps:
      # 1) Checkout
      - name: Checkout repository
        uses: actions/checkout@v4

      # 2) Chrome à¸ªà¸³à¸«à¸£à¸±à¸š Selenium (à¸–à¹‰à¸² scrap1.py à¹ƒà¸Šà¹‰)
      - name: Setup Google Chrome
        uses: browser-actions/setup-chrome@v1

      # 3) Python
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      # 4) Dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas selenium webdriver-manager
          pip install google-api-python-client google-auth google-auth-httplib2

      # 5) Run scraper
      - name: Run scraper
        run: python scrap1.py

      # 6) Debug à¹„à¸Ÿà¸¥à¹Œ CSV
      - name: List files
        run: ls -lh
      - name: Show CSV head
        run: |
          test -f "${{ env.CSV_OUT }}" || (echo "CSV not found!" && exit 1)
          head -n 20 "${{ env.CSV_OUT }}"

      # 7) à¹€à¸‚à¸µà¸¢à¸™à¹„à¸Ÿà¸¥à¹Œ service account à¸ˆà¸²à¸ secret
      - name: Write SA JSON
        run: |
          echo '${{ secrets.SERVICE_ID }}' > sa.json

      # 8) à¸­à¸±à¸›à¹€à¸”à¸•à¹„à¸Ÿà¸¥à¹Œà¹€à¸”à¸´à¸¡à¹ƒà¸™ My Drive à¸”à¹‰à¸§à¸¢ Drive API (update by fileId)
      - name: Append-merge CSV into existing Drive file
        env:
          TMD_FILE_ID: ${{ secrets.TMD_FILE_ID }}
          CSV_OUT: ${{ env.CSV_OUT }}
        run: |
          python - <<'PY'
          import os, sys, io
          import pandas as pd
          from google.oauth2 import service_account
          from googleapiclient.discovery import build
          from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload
          from googleapiclient.errors import HttpError
      
          file_id = os.environ.get("TMD_FILE_ID", "").strip()
          csv_path = os.environ.get("CSV_OUT", "tmd_7day_forecast_today.csv")
      
          if not file_id:
            print("âŒ TMD_FILE_ID is empty or missing."); sys.exit(2)
          if not os.path.exists(csv_path):
            print(f"âŒ CSV not found: {csv_path}"); sys.exit(2)
      
          # à¹ƒà¸Šà¹‰ scope à¹à¸šà¸šà¸à¸§à¹‰à¸²à¸‡à¹€à¸žà¸·à¹ˆà¸­à¹€à¸¥à¸µà¹ˆà¸¢à¸‡ policy à¸šà¸²à¸‡à¸­à¸‡à¸„à¹Œà¸à¸£
          SCOPES = ["https://www.googleapis.com/auth/drive"]
          creds = service_account.Credentials.from_service_account_file("sa.json", scopes=SCOPES)
          drive = build("drive", "v3", credentials=creds, cache_discovery=False)
      
          # 1) à¸•à¸£à¸§à¸ˆà¸à¸²à¸£à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡à¹„à¸Ÿà¸¥à¹Œà¸›à¸¥à¸²à¸¢à¸—à¸²à¸‡
          try:
            meta = drive.files().get(fileId=file_id, fields="id,name,mimeType,owners(emailAddress)").execute()
            print(f"âœ… Access OK: {meta['name']} ({meta['id']}) | owner={meta['owners'][0]['emailAddress']}")
            if meta["mimeType"].startswith("application/vnd.google-apps"):
              print("âŒ Target is a Google Doc/Sheet. This step supports only CSV binary files.", file=sys.stderr)
              sys.exit(3)
          except HttpError as e:
            print(f"âŒ Cannot access fileId={file_id}. Detail: {e}"); sys.exit(3)
      
          # 2) à¹‚à¸«à¸¥à¸”à¹„à¸Ÿà¸¥à¹Œà¹€à¸”à¸´à¸¡ (à¸–à¹‰à¸²à¸­à¹ˆà¸²à¸™ encoding à¸›à¸à¸•à¸´à¹„à¸¡à¹ˆà¹„à¸”à¹‰ à¸ˆà¸°à¸¥à¸­à¸‡ utf-8-sig)
          old_df = pd.DataFrame()
          try:
            buf = io.BytesIO()
            req = drive.files().get_media(fileId=file_id)
            dl = MediaIoBaseDownload(buf, req)
            done = False
            while not done:
              status, done = dl.next_chunk()
            buf.seek(0)
            try:
              old_df = pd.read_csv(buf)
            except Exception:
              buf.seek(0)
              old_df = pd.read_csv(buf, encoding="utf-8-sig")
            print(f"â„¹ï¸ Loaded existing rows: {len(old_df)}")
          except HttpError as e:
            # à¸–à¹‰à¸²à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸”à¹„à¸¡à¹ˆà¹„à¸”à¹‰ (à¹€à¸Šà¹ˆà¸™à¹„à¸Ÿà¸¥à¹Œà¸§à¹ˆà¸²à¸‡à¹€à¸žà¸´à¹ˆà¸‡à¸ªà¸£à¹‰à¸²à¸‡) à¸ˆà¸°à¸–à¸·à¸­à¸§à¹ˆà¸²à¹€à¸à¹ˆà¸²à¹€à¸›à¹‡à¸™à¸„à¹ˆà¸²à¸§à¹ˆà¸²à¸‡
            print(f"âš ï¸ Could not download existing CSV, assume empty. Detail: {e}")
      
          # 3) à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹ƒà¸«à¸¡à¹ˆà¸ˆà¸²à¸à¸£à¸­à¸šà¸™à¸µà¹‰
          try:
            new_df = pd.read_csv(csv_path)
            print(f"â„¹ï¸ New rows: {len(new_df)}")
          except Exception as e:
            print(f"âŒ Failed to read local CSV: {e}"); sys.exit(2)
      
          # 4) à¸£à¸§à¸¡ + à¸¥à¸šà¸‹à¹‰à¸³à¸­à¸¢à¹ˆà¸²à¸‡à¸›à¸¥à¸­à¸”à¸ à¸±à¸¢
          merged = pd.concat([old_df, new_df], ignore_index=True)
      
          # à¸–à¹‰à¸²à¸¡à¸µà¸„à¸µà¸¢à¹Œ Province + DateTime à¹ƒà¸«à¹‰à¹ƒà¸Šà¹‰à¹€à¸›à¹‡à¸™ unique key
          if {"Province", "DateTime"}.issubset(merged.columns):
            merged.drop_duplicates(subset=["Province", "DateTime"], keep="last", inplace=True)
            # à¸ˆà¸±à¸”à¹€à¸£à¸µà¸¢à¸‡à¹€à¸§à¸¥à¸²à¸–à¹‰à¸²à¸žà¸²à¸£à¹Œà¸ªà¹„à¸”à¹‰
            try:
              merged["DateTime"] = pd.to_datetime(merged["DateTime"], errors="coerce")
              merged.sort_values("DateTime", inplace=True)
            except Exception:
              pass
          else:
            merged.drop_duplicates(keep="last", inplace=True)
      
          tmp_path = "__merged_out.csv"
          merged.to_csv(tmp_path, index=False, encoding="utf-8-sig")
          print(f"ðŸ“ Merged rows total: {len(merged)}")
      
          # 5) à¸­à¸±à¸›à¹€à¸”à¸•à¸—à¸±à¸šà¹„à¸Ÿà¸¥à¹Œà¹€à¸”à¸´à¸¡à¸”à¹‰à¸§à¸¢à¹€à¸™à¸·à¹‰à¸­à¸«à¸² merged
          try:
            media = MediaFileUpload(tmp_path, mimetype="text/csv", resumable=False)
            updated = drive.files().update(
              fileId=file_id,
              media_body=media,
              fields="id,name,modifiedTime,size"
            ).execute()
            print(f"âœ… Updated (appended): {updated['name']} | size={updated.get('size')} | modifiedTime={updated['modifiedTime']}")
          except HttpError as e:
            print(f"âŒ Update failed: {e}"); sys.exit(4)
          PY
