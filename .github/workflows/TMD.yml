name: TMD Scraper

on:
  schedule:
    - cron: "0 13 * * *"   # 20:00 ‡πÑ‡∏ó‡∏¢ (13:00 UTC)
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    env:
      TZ: Asia/Bangkok
      CSV_OUT: tmd_7day_forecast_today.csv

    steps:
      # 1) Checkout
      - name: Checkout repository
        uses: actions/checkout@v4

      # 2) Chrome ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Selenium
      - name: Setup Google Chrome
        uses: browser-actions/setup-chrome@v1

      # 3) Python
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      # 4) Dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas selenium webdriver-manager
          pip install google-api-python-client google-auth google-auth-httplib2

      # 5) Run scraper
      - name: Run scraper
        run: python scrap1.py

      # 6) Debug ‡πÑ‡∏ü‡∏•‡πå CSV
      - name: List files
        run: ls -lh
      - name: Show CSV head
        run: |
          test -f "${{ env.CSV_OUT }}" || (echo "CSV not found!" && exit 1)
          head -n 20 "${{ env.CSV_OUT }}"

      # 7) ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå service account ‡∏à‡∏≤‡∏Å secret
      - name: Write SA JSON
        run: |
          echo '${{ secrets.SERVICE_ID }}' > sa.json

      # 8) Append + merge ‡πÅ‡∏•‡πâ‡∏ß‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°‡∏ö‡∏ô Drive (‡∏£‡∏±‡∏Å‡∏©‡∏≤ DateTime ‡πÄ‡∏î‡∏¥‡∏°)
      - name: Append-merge CSV into existing Drive file
        env:
          TMD_FILE_ID: ${{ secrets.TMD_FILE_ID }}
          CSV_OUT: ${{ env.CSV_OUT }}
        run: |
          python - <<'PY'
          import os, sys, io
          import pandas as pd
          from google.oauth2 import service_account
          from googleapiclient.discovery import build
          from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload
          from googleapiclient.errors import HttpError

          file_id = os.environ.get("TMD_FILE_ID", "").strip()
          csv_path = os.environ.get("CSV_OUT", "tmd_7day_forecast_today.csv")

          if not file_id:
            print("‚ùå TMD_FILE_ID is empty or missing."); sys.exit(2)
          if not os.path.exists(csv_path):
            print(f"‚ùå CSV not found: {csv_path}"); sys.exit(2)

          SCOPES = ["https://www.googleapis.com/auth/drive"]
          creds = service_account.Credentials.from_service_account_file("sa.json", scopes=SCOPES)
          drive = build("drive", "v3", credentials=creds, cache_discovery=False)

          # 1) ‡∏ï‡∏£‡∏ß‡∏à‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á
          try:
            meta = drive.files().get(fileId=file_id, fields="id,name,mimeType,owners(emailAddress)").execute()
            print(f"‚úÖ Access OK: {meta['name']} ({meta['id']}) | owner={meta['owners'][0]['emailAddress']}")
            if meta["mimeType"].startswith("application/vnd.google-apps"):
              print("‚ùå Target is a Google Doc/Sheet. Only CSV files are supported.", file=sys.stderr)
              sys.exit(3)
          except HttpError as e:
            print(f"‚ùå Cannot access fileId={file_id}. Detail: {e}"); sys.exit(3)

          # 2) ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°
          old_df = pd.DataFrame()
          try:
            buf = io.BytesIO()
            req = drive.files().get_media(fileId=file_id)
            dl = MediaIoBaseDownload(buf, req)
            done = False
            while not done:
              status, done = dl.next_chunk()
            buf.seek(0)
            try:
              old_df = pd.read_csv(buf)
            except Exception:
              buf.seek(0)
              old_df = pd.read_csv(buf, encoding="utf-8-sig")
            print(f"‚ÑπÔ∏è Loaded existing rows: {len(old_df)}")
          except HttpError as e:
            print(f"‚ö†Ô∏è Could not download existing CSV, assume empty. Detail: {e}")

          # 3) ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
          try:
            new_df = pd.read_csv(csv_path)
            print(f"‚ÑπÔ∏è New rows: {len(new_df)}")
          except Exception as e:
            print(f"‚ùå Failed to read local CSV: {e}"); sys.exit(2)

          # 4) ‡∏£‡∏ß‡∏° + ‡∏•‡∏ö‡∏ã‡πâ‡∏≥ (‡∏£‡∏±‡∏Å‡∏©‡∏≤ DateTime ‡πÄ‡∏õ‡πá‡∏ô string)
          merged = pd.concat([old_df, new_df], ignore_index=True)

          if "DateTime" in merged.columns:
            merged["DateTime"] = merged["DateTime"].astype(str)

          if {"Province", "DateTime"}.issubset(merged.columns):
            merged.drop_duplicates(subset=["Province", "DateTime"], keep="last", inplace=True)
            # ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ä‡πà‡∏ß‡∏¢ (‡πÑ‡∏°‡πà‡πÅ‡∏ï‡∏∞‡∏Ñ‡πà‡∏≤ DateTime ‡πÄ‡∏î‡∏¥‡∏°)
            sort_key = pd.to_datetime(merged["DateTime"], errors="coerce")
            merged = merged.assign(__sort_dt=sort_key).sort_values(["Province","__sort_dt"]).drop(columns="__sort_dt")
          else:
            merged.drop_duplicates(keep="last", inplace=True)

          tmp_path = "__merged_out.csv"
          merged.to_csv(tmp_path, index=False, encoding="utf-8-sig")
          print(f"üìù Merged rows total: {len(merged)}")

          # 5) ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ó‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°
          try:
            media = MediaFileUpload(tmp_path, mimetype="text/csv", resumable=False)
            updated = drive.files().update(
              fileId=file_id,
              media_body=media,
              fields="id,name,modifiedTime,size"
            ).execute()
            print(f"‚úÖ Updated (appended): {updated['name']} | size={updated.get('size')} | modifiedTime={updated['modifiedTime']}")
          except HttpError as e:
            print(f"‚ùå Update failed: {e}"); sys.exit(4)
          PY
